Demo: Create an EKS Cluster with eksctl:
========================================
Prerequisites:
1. An AWS account with permissions to create EKS clusters (IAM + EC2 + VPC + CloudFormation)
2. aws CLI configured (aws configure) with credentials for the account/region
3. eksctl installed
4. kubectl installed and in PATH. kubectl version should be within one minor version of the cluster

[Optional but recommended) helm for installing ingress controllers or other add-ons.

Quick verification commands (run before starting the cluster demo)
 > aws sts get-caller-identity â€” verifies AWS credentials.
 > eksctl version â€” verifies eksctl.
 > kubectl version --client â€” verifies kubectl.

Steps:
------
1. Open CloudShell from the AWS dashboard - and install the eksctl

2. Download the latest eksctl binary: curl --silent --location "https://github.com/eksctl-io/eksctl/releases/latest/download/eksctl_$(uname -s)_amd64.tar.gz" -o eksctl.tar.gz

3. Extract the binary: tar -xzf eksctl.tar.gz

4. Move eksctl to the directory in your PATH: sudo mv eksctl /usr/local/bin

5. Verify installation: eksctl version

6. Make sure kubectl is installed: kubectl version --client

7. Make sure AWS CLI is working: aws sts get-caller-identity
[This confirms CloudShell is authenticated to your AWS account.
Output will include your Account ID, ARN, and User ID]

8. Once eksctl is installed, you can create you EKS cluster with:

eksctl create cluster \
  --name demo-cluster \
  --region us-east-2 \
  --nodegroup-name demo-nodes \
  --node-type t3.medium \
  --nodes 2

Explanation:

| Part                          | Meaning                                  |
| ----------------------------- | ---------------------------------------- |
| `eksctl create cluster`       | Tells eksctl to create a new EKS cluster |
| `--name demo-cluster`         | Name of your Kubernetes cluster          |
| `--region us-east-1`          | AWS region to deploy to                  |
| `--nodegroup-name demo-nodes` | Logical group name for the worker nodes  |
| `--node-type t3.medium`       | EC2 instance type for each worker node   |
| `--nodes 2`                   | Number of worker nodes in the group      |

[*** make sure this will take time to complete the process might take few minutes also]

9. now, confirm the same by running command: kubectl get nodes [we can see 2 nodes has been created]

10. Make sure to delete once after deleting this resource: eksctl delete cluster --name demo-cluster --region us-east-2

What happens when you run this:
-------------------------------
1. eksctl will:
 > Delete the EKS control plane.
 > Terminate all worker nodes (EC2 instances).
 > Remove the Auto Scaling Group.
 > Delete the VPC, subnets, route tables, Internet/NAT gateways it created.
 > Remove IAM roles and security groups linked to the cluster.

2. The process takes 10â€“15 minutes because AWS needs to clean up networking and EC2 resources.

FYI***:
=======
When we are creating EKS cluster using eksctl, there are a lot resources will get created in our AWS account; here are the lists

1. Control Plane (Managed by AWS)
These components are invisible to you in terms of infrastructure (AWS manages them), but are still part of what you â€œgetâ€ when creating a cluster.
 > Kubernetes API server â€“ Handles all kubectl commands.
 > etcd â€“ Stores cluster state and configuration.
 > Controller Manager â€“ Runs background tasks (e.g., deployments, scaling).
 > Scheduler â€“ Decides which node runs which pod.

ðŸ“Œ Important: The control plane does not run on your EC2 nodes â€” itâ€™s fully managed and highly available in AWS.

2. AWS Infrastructure Resources Created by eksctl
When eksctl create cluster runs, it talks to AWS APIs and provisions:

a) VPC & Networking
 > VPC â€“ Virtual Private Cloud to isolate your cluster.
 > Subnets â€“ Usually 2â€“3 private subnets and 2â€“3 public subnets.
 > Internet Gateway â€“ Allows resources in public subnets to access the internet.
 > NAT Gateway â€“ Allows resources in private subnets to reach the internet.
 > Route Tables â€“ Direct network traffic inside the VPC.

Why: Your worker nodes and pods need network connectivity.

b) IAM Roles & Policies
 > EKS Cluster Role â€“ Gives the control plane permissions to manage AWS resources.
 > Node Instance Role â€“ Attached to EC2 nodes for pulling images from ECR, writing to CloudWatch logs, etc.

Optional: Additional IAM roles for add-ons (like AWS Load Balancer Controller, EBS CSI Driver).

c) Security Groups
 > Cluster Security Group â€“ Controls access to the EKS control plane.
 > Node Security Group â€“ Controls traffic between nodes and from nodes to the control plane.

d) EC2 Worker Nodes
 > Auto Scaling Group (ASG) â€“ A group of EC2 instances that act as Kubernetes worker nodes.
 > Launch Template â€“ Defines the AMI, instance type, and config for these nodes.
 > Amazon EKS-Optimized AMI â€“ Pre-installed with kubelet and containerd.

e) EKS Cluster Object
 > The logical EKS cluster itself (shown in AWS Console â†’ EKS).
 > Contains the endpoint for your kubectl commands.

f) Add-ons
By default, EKS installs some Kubernetes system add-ons:
 > CoreDNS â€“ Internal DNS for pods and services.
 > kube-proxy â€“ Handles pod-to-pod networking.
 > Amazon VPC CNI Plugin â€“ Allows pods to get IP addresses from your VPC.

In Real-life:
-------------
When you say â€œcreate a clusterâ€ in eksctl, youâ€™re not just creating Kubernetes â€” youâ€™re also spinning up:

 > A mini data center (VPC, networking)
 > A team of servers (EC2 nodes)
 > Security guards (Security Groups, IAM)
 > And managers (control plane services) â€” all talking to each other.



























